üöÄ AI-Powered Data Pipeline Assistant
Hybrid RAG + Agents for Modern Data Engineering Workflows
üìå Overview

AI-Powered Data Pipeline Assistant is an intelligent debugging and automation system designed for data engineers.
It analyzes logs, SQL queries, Airflow DAGs, Spark jobs, and pipeline configs ‚Äî and provides root-cause analysis, fix suggestions, and auto-generated remediation steps.

Built with a Hybrid RAG architecture, the system combines:

‚úÖ Static curated knowledge stored in Supabase (pgvector)
‚úÖ Dynamic per-request vectors stored locally in FAISS (in-memory)
‚úÖ On-demand retrieval from official docs, StackOverflow, and internal repos
‚úÖ Agents that execute real tools (SQL validator, config checker, DAG parser, log analyzer)

This makes the assistant far more powerful than a normal chatbot.

üß† Key Features
üîç 1. Intelligent Data Pipeline Debugging

Upload logs, SQL, config files, or DAGs ‚Äî the system identifies the problem and proposes actionable fixes.

üîÑ 2. Hybrid RAG Retrieval

Fast vector search from curated knowledge in Supabase

Dynamic vector search via FAISS (session temporary store)

Fresh data from official docs & StackOverflow

ü§ñ 3. Agent-Based Automation

Tools include:

SQL Analysis Tool

Data Schema Comparator

Airflow DAG Linter

Spark Job Troubleshooter

Log Pattern Analyzer

Pipeline Health Reporter

üìö 4. Lightweight Knowledge Base

Short curated chunks:

Airflow errors

Spark exceptions

Common ETL design patterns

SQL tuning tips

Cloud warehouse best practices (Snowflake/BigQuery)

Minimal size ‚Üí minimal cost.

üì¶ 5. Real-Time Streaming Responses

PubNub + Streamlit for a responsive UI.

üèóÔ∏è System Architecture
üî∑ High-Level Diagram (Hybrid RAG)
flowchart TB

    %% Users & Frontend
    U[User] --> S[Streamlit Frontend]
    S --> |HTTP Requests| F[FastAPI Backend]

    %% AI Core Services
    F --> O[OpenAI API]
    F --> AG[Agent Orchestrator]

    %% RAG Engine
    AG --> R[RAG Engine]

    %% External Data Sources
    R --> SO[StackOverflow API]
    R --> OD[Official Docs Fetcher]
    R --> ID[Internal Repos]

    %% Real-time Communication
    F --> P[PubNub WebSocket]
    P --> S

    %% Database Layer
    subgraph "Supabase Database"
        T[tasks]
        CH[chat_history] 
        L[logs]
        TE[tool_executions]

        subgraph "Persistent KB (pgvector)"
            KB[knowledge_base]
        end
    end

    subgraph "Local In-Memory Vector Store"
        KB_TMP[FAISS Temporary Vectors<br/>- session scoped<br/>- auto-cleans]
    end

    %% Backend writes/reads DB
    F --> T
    F --> CH
    F --> L
    F --> TE

    %% Hybrid RAG flow
    R --> KB
    KB --> |Static Vector Search| R
    R --> KB_TMP
    KB_TMP --> |Dynamic Vector Search| R

    %% Styling
    classDef frontend fill:#d9f1ff
    classDef backend fill:#efe1ff
    classDef database fill:#e7ffe7
    classDef external fill:#fff4db
    classDef temp fill:#ffe7e7

    class U,S frontend
    class F,AG,R,P backend
    class T,CH,L,TE,KB temp
    class KB database
    class KB_TMP temp
    class SO,OD,ID external

üîç How Hybrid RAG Works
1. Static Retrieval (Supabase pgvector)

Stores curated, stable chunks

Small & cheap to maintain

Fast pgvector search

Used for:

Common ETL fixes

SQL tuning patterns

Known Airflow/Spark error patterns

2. Dynamic Retrieval (FAISS In-Memory)

Used when user uploads:

Logs

DAG files

SQL scripts

Config YAML

Error messages

Or when the system fetches:

StackOverflow answers

Official docs

Internal repos

‚û°Ô∏è FAISS stores vectors only for the session
‚û°Ô∏è Cleared after task ends
‚û°Ô∏è Zero cost, ultra-fast search

üóÑÔ∏è Database Schema (Supabase)
1. knowledge_base (pgvector)

Minimal curated chunks.

2. kb_temp (session-level FAISS)

Not stored in DB, kept in memory.

3. tasks

Tracks long-running jobs.

4. chat_history

Maintains conversation context.

5. tool_executions

Every agent call is logged:

tool used

input

output

latency

6. logs

System + user logs for debugging.

üß™ Hallucination Mitigation / Evaluation

Your evaluator module will cover:

1. Source Score Evaluation

"Is answer backed by KB?"

Scores from 0‚Äì1

< 0.6 triggers re-query

2. Consistency Check

LLM answers twice with different seeds ‚Üí compare embeddings.

3. Tool-grounded verification

If answer is about:

SQL ‚Üí validate with PostgreSQL

DAG ‚Üí parse AST

Logs ‚Üí regex error pattern matching

4. RAG Contrastive Evaluation

Ask LLM:

‚ÄúWhich chunk(s) from retrieval support the answer?‚Äù

Compare with actual retrieval list

5. Refusal Logic

If retrieval confidence is low ‚Üí respond:

‚ÄúI don‚Äôt have enough information. Please upload logs or DAG files.‚Äù

üß© User Interface
Streamlit Screens:

Chat Interface

upload logs/sql/dag/config

select pipeline type (Airflow/Spark/dbt)

Debug Panel
Shows:

retrieved docs

agent tool calls

SQL validation output

DAG dependency graph

Pipeline Report
Auto-generated:

root cause summary

severity

recommended fixes

validation steps

Knowledge Base Inspector
Shows chunks from Supabase + temp FAISS vectors.

‚öôÔ∏è Project Structure
ai-pipeline-assistant/
‚îÇ
‚îú‚îÄ‚îÄ backend/
‚îÇ   ‚îú‚îÄ‚îÄ main.py (FastAPI)
‚îÇ   ‚îú‚îÄ‚îÄ rag_engine/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ static_retriever.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dynamic_retriever.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ hybrid_rag.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ chunking/
‚îÇ   ‚îú‚îÄ‚îÄ agents/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sql_checker.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dag_linter.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ log_analyzer.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ report_generator.py
‚îÇ   ‚îú‚îÄ‚îÄ vectorstores/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ supabase_pgvector.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ faiss_temp_store.py
‚îÇ   ‚îú‚îÄ‚îÄ db/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ supabase_client.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ queries.sql
‚îÇ   ‚îî‚îÄ‚îÄ eval/
‚îÇ       ‚îú‚îÄ‚îÄ consistency_test.py
‚îÇ       ‚îú‚îÄ‚îÄ source_score.py
‚îÇ       ‚îî‚îÄ‚îÄ tool_validation.py
‚îÇ
‚îú‚îÄ‚îÄ frontend/
‚îÇ   ‚îú‚îÄ‚îÄ app.py (Streamlit)
‚îÇ   ‚îú‚îÄ‚îÄ components/
‚îÇ   ‚îî‚îÄ‚îÄ styles/
‚îÇ
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îú‚îÄ‚îÄ architecture.md
‚îÇ   ‚îú‚îÄ‚îÄ diagrams/
‚îÇ   ‚îî‚îÄ‚îÄ evaluations.md
‚îÇ
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ ingest_docs.py
‚îÇ   ‚îú‚îÄ‚îÄ scrape_stackoverflow.py
‚îÇ   ‚îî‚îÄ‚îÄ populate_kb.py
‚îÇ
‚îî‚îÄ‚îÄ README.md

üöÄ Running Locally
1. Start backend
cd backend
uvicorn main:app --reload

2. Start Streamlit UI
cd frontend
streamlit run app.py

üì¶ Deployment Options
Option A: Render (Free)

Frontend: Streamlit

Backend: FastAPI

Supabase: hosted

Option B: Docker + Railway

One-click PaaS

WebSocket support

Option C: Local Demo

FAISS + Supabase

Perfect for your bootcamp capstone demo

üõ£Ô∏è Future Enhancements
üî• 1. Automatic pipeline repair

Let agent automatically create a PR with:

DAG fixes

SQL fixes

Config fixes

üìä 2. Pipeline Observability Dashboard

Metrics from Airflow/Spark ‚Üí LLM analysis.

üß© 3. Plugin Marketplace

Add new tools:

dbt model checker

Kafka lag analyzer

Data Quality profiler

ü§ù 4. Collaboration Mode

Multiple users share same session & temp vector DB.

üéØ Why This Project Is Unique

Most LLM debugging tools are simple chatbots.
Your system is not. It performs:

real tool execution

hybrid dynamic/static retrieval

live scraping of real sources

deep integration with data engineering workflows

full evaluation suite to reduce hallucinations

enterprise-style architecture

This is a capstone worthy of a Senior Data Engineer or ML Engineer.